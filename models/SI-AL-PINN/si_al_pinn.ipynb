{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebook for the Implemntation and training of the network based on the papet\n",
    "\"Research on the thermal-fluid coupling in the growth process of Czochralski silicon single crystals based on an improved physics-informed neural network\" \n",
    "\n",
    "Next steps is in handling the CFD simulations data either from COMSOL or snythetic data generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Introduction & Theory\n",
    "# Research on Thermal-Fluid Coupling in Czochralski Crystal Growth via SI-LB-PINN\n",
    "\n",
    "**SI-LB-PINN** (Spatial-Information + Loss-Balancing Physics-Informed Neural Network) as described in DOI: [10.1063/5.0271778](https://doi.org/10.1063/5.0271778).\n",
    "\n",
    "### The Czochralski (CZ) Process\n",
    "The CZ process is the primary method for growing single-crystal silicon. It involves complex thermal-fluid coupling:\n",
    "1. **Navier-Stokes Equations:** Govern the melt flow.\n",
    "2. **Energy Equation:** Governs heat transfer.\n",
    "3. **Boussinesq Approximation:** Usually used to couple temperature and flow via buoyancy.\n",
    "\n",
    "### Key Innovations\n",
    "1. **Spatial-Information (SI) Layer:** Standard MLPs often struggle with high-frequency spatial features. The SI layer injects raw coordinates $(x, y)$ into every hidden layer using learned gating mechanisms:\n",
    "   $$H_{l} = \\sigma( (1-Z) \\odot M(X) + Z \\odot N(X) + W H_{l-1} + b )$$\n",
    "2. **Loss-Balancing (LB):** PINNs involve multiple loss terms (Continuity, Momentum, Energy, BCs). This implementation uses **Uncertainty Weighting** (adaptive weights) to automatically balance these terms during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and Hyperparameters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Physics & Network Params\n",
    "config = {\n",
    "    \"nu\": 1e-3,      # Kinematic viscosity\n",
    "    \"kappa\": 1e-3,   # Thermal diffusivity\n",
    "    \"lr\": 0.001,\n",
    "    \"epochs\": 5000,\n",
    "    \"hidden_layers\": 5,\n",
    "    \"hidden_units\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Spatial-Information (SI) Architecture\n",
    "\n",
    "class SILayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects spatial coordinates into the hidden state to preserve \n",
    "    spatial gradients better than standard MLPs.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, input_dim=2):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_features, out_features)\n",
    "        self.M = nn.Sequential(nn.Linear(input_dim, out_features), nn.Tanh())\n",
    "        self.N = nn.Sequential(nn.Linear(input_dim, out_features), nn.Tanh())\n",
    "        self.Z = nn.Sequential(nn.Linear(in_features, out_features), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, H_prev, X):\n",
    "        # Mx and Nx are spatial feature encoders\n",
    "        Mx = self.M(X)\n",
    "        Nx = self.N(X)\n",
    "        # Zx is a gating mechanism based on the previous hidden state\n",
    "        Zx = self.Z(H_prev)\n",
    "        # Combine using the gate\n",
    "        H = (1 - Zx) * Mx + Zx * Nx + self.lin(H_prev)\n",
    "        return torch.tanh(H)\n",
    "\n",
    "class SI_LB_PINN(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_units=128, hidden_layers=5, output_dim=4):\n",
    "        super().__init__()\n",
    "        self.input_lin = nn.Linear(input_dim, hidden_units)\n",
    "        self.silayers = nn.ModuleList([\n",
    "            SILayer(hidden_units, hidden_units, input_dim) \n",
    "            for _ in range(hidden_layers)\n",
    "        ])\n",
    "        self.out = nn.Linear(hidden_units, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H = torch.tanh(self.input_lin(x))\n",
    "        for layer in self.silayers:\n",
    "            H = layer(H, x)\n",
    "        return self.out(H) # Returns [u, v, T, p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the Physics (PDE Residuals) the steady-state Navier-Stokes and Energy equations.\n",
    "\n",
    "def pde_residuals(model, x, nu, kappa):\n",
    "    x.requires_grad_(True)\n",
    "    out = model(x)\n",
    "    u, v, T, p = out[:, 0:1], out[:, 1:2], out[:, 2:3], out[:, 3:4]\n",
    "\n",
    "    # Helper for gradients\n",
    "    def grad(phi, x):\n",
    "        return autograd.grad(phi, x, grad_outputs=torch.ones_like(phi), \n",
    "                             create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "    du = grad(u, x)\n",
    "    dv = grad(v, x)\n",
    "    dT = grad(T, x)\n",
    "    dp = grad(p, x)\n",
    "\n",
    "    u_x, u_y = du[:, 0:1], du[:, 1:2]\n",
    "    v_x, v_y = dv[:, 0:1], dv[:, 1:2]\n",
    "    T_x, T_y = dT[:, 0:1], dT[:, 1:2]\n",
    "    p_x, p_y = dp[:, 0:1], dp[:, 1:2]\n",
    "\n",
    "    # Second derivatives\n",
    "    u_xx = grad(u_x, x)[:, 0:1]\n",
    "    u_yy = grad(u_y, x)[:, 1:2]\n",
    "    v_xx = grad(v_x, x)[:, 0:1]\n",
    "    v_yy = grad(v_y, x)[:, 1:2]\n",
    "    T_xx = grad(T_x, x)[:, 0:1]\n",
    "    T_yy = grad(T_y, x)[:, 1:2]\n",
    "\n",
    "    # Residuals\n",
    "    r_cont = u_x + v_y\n",
    "    r_momx = (u*u_x + v*u_y) + p_x - nu*(u_xx + u_yy)\n",
    "    r_momy = (u*v_x + v*v_y) + p_y - nu*(v_xx + v_yy)\n",
    "    r_energy = (u*T_x + v*T_y) - kappa*(T_xx + T_yy)\n",
    "\n",
    "    return r_cont, r_momx, r_momy, r_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adaptive Training Loop\n",
    "model = SI_LB_PINN().to(device)\n",
    "# 7 terms: 4 PDE + 3 Boundary Conditions\n",
    "log_vars = nn.Parameter(torch.zeros(7, device=device), requires_grad=True)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + [log_vars], lr=config['lr'])\n",
    "\n",
    "# Placeholder for boundary data sampling functions (defined in your code)\n",
    "# ... insert sample_interior, sample_boundary, bc_values functions here ...\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1. PDE Loss\n",
    "    X_int = sample_interior(2000)\n",
    "    res = pde_residuals(model, X_int, config['nu'], config['kappa'])\n",
    "    pde_losses = torch.stack([torch.mean(r**2) for r in res])\n",
    "    \n",
    "    # 2. BC Loss\n",
    "    X_bc = sample_boundary(500)\n",
    "    pred_bc = model(X_bc)\n",
    "    true_bc = bc_values(X_bc)\n",
    "    bc_losses = torch.stack([\n",
    "        torch.mean((pred_bc[:, 0:1] - true_bc['u'])**2),\n",
    "        torch.mean((pred_bc[:, 1:2] - true_bc['v'])**2),\n",
    "        torch.mean((pred_bc[:, 2:3] - true_bc['T'])**2)\n",
    "    ])\n",
    "    \n",
    "    # 3. Total Balanced Loss\n",
    "    all_losses = torch.cat([pde_losses, bc_losses])\n",
    "    loss = torch.sum(torch.exp(-log_vars) * all_losses + log_vars)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {loss.item():.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training on CFD Data (COMSOL)\n",
    "\n",
    "The paper improves PINN performance by using data-informed training. Here is how to handle that.\n",
    "\n",
    "### 1. Where to obtain Crystal Growth Data?\n",
    "*   **Academic Databases:** Search for the **\"Czochralski benchmark\"** dataset. There are classic papers (e.g., Wheeler, 1990) that provide standard solutions for silicon melt.\n",
    "*   **COMSOL Application Gallery:** Look for the **\"Czochralski Crystal Growth\"** model in the Heat Transfer or CFD module.\n",
    "*   **OpenFOAM:** Use the `buoyantSimpleFoam` solver to simulate a rotating cylinder in a heated crucible.\n",
    "\n",
    "### 2. How to export from COMSOL for PINNs\n",
    "To use COMSOL data in PyTorch:\n",
    "1.  Run your simulation in COMSOL.\n",
    "2.  Go to **Results > Export > Data**.\n",
    "3.  Select the entire domain.\n",
    "4.  Export coordinates (`x, y`) and variables (`u, v, T, p`).\n",
    "5.  Set the format to **CSV** or **Text**.\n",
    "6.  **Crucial:** Ensure the data is nondimensionalized to match your PINN units (usually mapping the geometry to a $[0, 1]$ range).\n",
    "\n",
    "### 3. Modifying the Code for Data Training\n",
    "adding  `L_data` term to your loss function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you loaded COMSOL data into tensors: X_data, U_data (u,v,T,p)\n",
    "def train_with_data(model, X_data, U_data):\n",
    "    # ... inside loop ...\n",
    "    pred_data = model(X_data)\n",
    "    L_data = torch.mean((pred_data - U_data)**2)\n",
    "    \n",
    "    # Add L_data to the balanced loss stack\n",
    "    # loss = sum(exp(-log_v)*L + log_v) + L_data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How to make Synthetic Data\n",
    "If you don't have COMSOL, you can generate \"pseudo-physical\" synthetic data using:\n",
    "1.  **Analytical Manufactured Solutions:** Define a function that looks like a melt (e.g., swirling vortices using stream functions) and calculate the source terms needed to make that function satisfy the PDE.\n",
    "2.  **Simplified Solvers:** Use a coarse-grid Finite Difference method in Python (`numpy` or `scipy`) to solve a 2D lid-driven cavity problem with a temperature gradient.\n",
    "3.  **Pre-trained PINN:** Train a standard PINN for 10,000 epochs, then use its output (filtered for high-confidence regions) as \"noisy labels\" for the SI-LB-PINN to show how the improved architecture converges faster/better.\n",
    "\n",
    "### Recommendations for the Czochralski Case:\n",
    "*   **Rotation:** In CZ growth, the crystal and crucible rotate. You must add the **Coriolis force** and **Centrifugal force** terms to your momentum residuals.\n",
    "*   **Moving Boundary:** The crystal grows over time. For a simple PINN, start with a **Fixed Domain** (steady state) before attempting a time-dependent moving boundary."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
